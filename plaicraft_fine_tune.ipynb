{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from transformers import AutoProcessor\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "# bnb = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_name = \"CraftJarvis/JarvisVLA-Qwen2-VL-7B\"\n",
    "model_causallm = Qwen2VLForConditionalGeneration.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16,\n",
    "                                                                 device_map=\"cuda\", quantization_config=bnb, attn_implementation=\"flash_attention_2\",)\n",
    "model_causallm.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_NAME = \"CraftJarvis/minecraft-vla-sft\"\n",
    "# raw_datasets = load_dataset(DATASET_NAME)\n",
    "# train_dataset = raw_datasets['train']\n",
    "# exemplar = train_dataset[0]\n",
    "# exemplar['conversations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to open global database:  /ubc/cs/research/ubc_ml/plaicraft/data/versioning/global_databases/version_continuous_audio_hdf5/5.8k_hours_5603_players_ids/global_database_validation.db\n",
      "trying to open global database:  /ubc/cs/research/ubc_ml/plaicraft/data/versioning/global_databases/version_continuous_audio_hdf5/5.8k_hours_5603_players_ids/global_database_validation.db\n"
     ]
    }
   ],
   "source": [
    "from data.plaicraft_dataset_fixed_window import PlaicraftMapDataset\n",
    "from data.plaicraft_raw_dataset_fixed_window import PlaicraftMapRawDataset\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PREFIX = \"/ubc/cs/research/ubc_ml/plaicraft/data\"\n",
    "GLOBAL_DATABASE_SUFFIX = \"versioning/global_databases/version_continuous_audio_hdf5/5.8k_hours_5603_players_ids/global_database_validation.db\"\n",
    "WINDOW_LENGTH = 50\n",
    "\n",
    "dataset = PlaicraftMapDataset(\n",
    "    player_names=[\"Dante\"],\n",
    "    dataset_path=os.path.join(DATA_PREFIX, \"processed\"),\n",
    "    modalities=[\"video\", \"audio_in\", \"audio_out\", \"action\"],\n",
    "    window_length_frames=WINDOW_LENGTH,\n",
    "    global_database_path=Path(os.path.join(DATA_PREFIX, GLOBAL_DATABASE_SUFFIX))\n",
    ")\n",
    "raw_dataset = PlaicraftMapRawDataset(\n",
    "    player_names=[\"Dante\"],\n",
    "    dataset_path=os.path.join(DATA_PREFIX, \"processed\"),\n",
    "    modalities=[\"video\", \"audio_in\", \"audio_out\", \"action\"],\n",
    "    window_length_frames=WINDOW_LENGTH,\n",
    "    global_database_path=Path(os.path.join(DATA_PREFIX, GLOBAL_DATABASE_SUFFIX))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[00:12:56] </span>tokenizer_type: qwen2_vl                                                                   <a href=\"file:///ubc/cs/research/plai-scratch/jason/JarvisVLA/plaicraft/mappers/action.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">action.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/plai-scratch/jason/JarvisVLA/plaicraft/mappers/action.py#244\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">244</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[00:12:56]\u001b[0m\u001b[2;36m \u001b[0mtokenizer_type: qwen2_vl                                                                   \u001b]8;id=801377;file:///ubc/cs/research/plai-scratch/jason/JarvisVLA/plaicraft/mappers/action.py\u001b\\\u001b[2maction.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=519787;file:///ubc/cs/research/plai-scratch/jason/JarvisVLA/plaicraft/mappers/action.py#244\u001b\\\u001b[2m244\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[00:12:56] </span>bases: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span><span style=\"font-weight: bold\">]</span>, camera_mu: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, n_camera_bins: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,          <a href=\"file:///ubc/cs/research/plai-scratch/jason/JarvisVLA/plaicraft/mappers/action.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">action.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/plai-scratch/jason/JarvisVLA/plaicraft/mappers/action.py#245\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">245</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>camera_binsize: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[00:12:56]\u001b[0m\u001b[2;36m \u001b[0mbases: \u001b[1m[\u001b[0m\u001b[1;36m10\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m21\u001b[0m, \u001b[1;36m21\u001b[0m\u001b[1m]\u001b[0m, camera_mu: \u001b[1;36m20\u001b[0m, n_camera_bins: \u001b[1;36m21\u001b[0m,          \u001b]8;id=52859;file:///ubc/cs/research/plai-scratch/jason/JarvisVLA/plaicraft/mappers/action.py\u001b\\\u001b[2maction.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=253425;file:///ubc/cs/research/plai-scratch/jason/JarvisVLA/plaicraft/mappers/action.py#245\u001b\\\u001b[2m245\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0mcamera_binsize: \u001b[1;36m1\u001b[0m                                                                          \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from plaicraft.keypress_autoencoder.model import KeyPressAutoencoder\n",
    "from plaicraft.keypress_autoencoder.constants import id_to_index, id_to_name\n",
    "\n",
    "\n",
    "# Overlay key presses\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "index_to_id = {index: key_id for key_id, index in id_to_index.items()}\n",
    "FIXED_MOUSE_BUTTONS = {\"left\": \"mouse_left\", \"right\": \"mouse_right\", \"scroll_up\": \"scroll_up\", \"scroll_down\": \"scroll_down\"}\n",
    "mouse_action_names = list(FIXED_MOUSE_BUTTONS.values())\n",
    "keyboard_indices = []\n",
    "mouse_indices = []\n",
    "for idx, key_id in index_to_id.items():\n",
    "    name = id_to_name.get(key_id, \"\")\n",
    "    if name in mouse_action_names:\n",
    "        mouse_indices.append(idx)\n",
    "    else:\n",
    "        keyboard_indices.append(idx)\n",
    "\n",
    "\n",
    "def load_keypress_autoencoder():\n",
    "    print(\"Loading KeyPressAutoencoder ...\")\n",
    "    keypress_ae_checkpoint_path = \"/ubc/cs/research/ubc_ml/plaicraft/plaicraft-data-preprocessing/encode_key_press/checkpoints/keyencoder_16_5_best_checkpoint.pt\"\n",
    "    keypress_ae = KeyPressAutoencoder(\n",
    "        input_dim=79,\n",
    "        latent_dim=16,\n",
    "        latent_seq_len=5,\n",
    "        original_seq_len=10,\n",
    "        num_gru_layers=2,\n",
    "        conv_dropout=0.1,\n",
    "        gru_dropout=0.1\n",
    "    ).to(device)\n",
    "    checkpoint = torch.load(keypress_ae_checkpoint_path, map_location=device)\n",
    "    keypress_ae.load_state_dict(checkpoint)\n",
    "    keypress_ae.eval()\n",
    "    return keypress_ae\n",
    "\n",
    "def decode_keypress(key_data):\n",
    "    \"\"\"Decode a chunk of keypresses using the KeyPressAutoencoder.\"\"\"\n",
    "    key_data = key_data.to(device)\n",
    "    num_chunks, chunk_size = key_data.shape[-1]//5, 5\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_chunks):\n",
    "            decoded_key = keypress_ae.decoder(key_data[..., i*chunk_size:(i+1)*chunk_size])\n",
    "            result.append(decoded_key.cpu().numpy())\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def keypress_tensor_to_text(key_data, keypress_threshold=0.5):\n",
    "    total_result = []\n",
    "    for i in range(len(key_data)):\n",
    "        result = []\n",
    "        union_pressed = np.any(key_data[i][0] >= keypress_threshold, axis=1)\n",
    "        pressed_indices = np.where(union_pressed)[0]\n",
    "        pressed_keyboard_indices = [p for p in pressed_indices if p in keyboard_indices]\n",
    "        pressed_mouse_indices = [p for p in pressed_indices if p in mouse_indices]\n",
    "        for key_idx in pressed_keyboard_indices:\n",
    "            key_id = index_to_id.get(key_idx, None)\n",
    "            key_name = id_to_name.get(key_id, f\"Key_{key_idx}\") if key_id is not None else f\"Key_{key_idx}\"\n",
    "            result.append(key_name)\n",
    "        for mouse_idx in pressed_mouse_indices:\n",
    "            mouse_id = index_to_id.get(mouse_idx, None)\n",
    "            mouse_name = id_to_name.get(mouse_id, f\"Mouse_{mouse_idx}\") if mouse_id is not None else f\"Mouse_{mouse_idx}\"\n",
    "            result.append(mouse_name)\n",
    "        total_result.append(result)\n",
    "    return total_result\n",
    "\n",
    "\n",
    "# keypress_ae = load_keypress_autoencoder()\n",
    "\n",
    "    \n",
    "import plaicraft.mappers\n",
    "from jarvisvla.inference import processor_wrapper\n",
    "\n",
    "processor_wrapper = processor_wrapper.ProcessorWrapper(processor, model_name=\"qwen2_vl\")\n",
    "action_tokenizer = plaicraft.mappers.PlaicraftActionTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypress_to_text(key_data, bin_size=5):\n",
    "    # Input Shape: [batch_size x seq_len x 79]\n",
    "    # Output Shape: [batch_size x seq_len x num_tokens]\n",
    "    assert key_data.shape[1] % bin_size == 0, \"Key data sequence length must be divisible by bin size.\" \n",
    "\n",
    "    result = []\n",
    "    for b in range(key_data.shape[0]):\n",
    "        result_b = []\n",
    "        for t in range(key_data.shape[1]):\n",
    "            result_t = []\n",
    "            nonzero_indices = (key_data[b, t] == 1).nonzero(as_tuple=False)\n",
    "            for idx in nonzero_indices:\n",
    "                key_id = index_to_id.get(idx.item(), None)\n",
    "                if key_id is not None:\n",
    "                    result_t.append(id_to_name[key_id])\n",
    "            result_b.append(result_t)\n",
    "\n",
    "        result_b_binned = []\n",
    "        for i_bin in range(0, len(result_b), bin_size):\n",
    "            result_t_binned = list(set(sum(result_b[i_bin:i_bin + bin_size], [])))\n",
    "            result_b_binned.append(result_t_binned)\n",
    "        result.append(result_b_binned)\n",
    "    return result\n",
    "\n",
    "key_data = test_data['action']['key_press'].flatten(1, 3)\n",
    "key_data_text = keypress_to_text(key_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to open global database:  /ubc/cs/research/ubc_ml/plaicraft/data/versioning/global_databases/version_continuous_audio_hdf5/5.8k_hours_5603_players_ids/global_database_validation.db\n",
      "[[], [], [], [], [], [], [], ['w'], ['w'], ['w'], ['w'], ['w'], ['w'], [], [], [], [], [], [], [], [], [], [], ['w'], ['w'], ['w'], ['w'], ['w'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['s'], ['s'], ['s'], [], [], [], [], []]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wtfff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m result_raw_text \u001b[38;5;241m=\u001b[39m keypress_to_text(key_press_raw)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_raw_text)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mwtfff\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# for i in range(0, 1000):\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m490\u001b[39m, \u001b[38;5;241m510\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wtfff' is not defined"
     ]
    }
   ],
   "source": [
    "from data.plaicraft_raw_dataset_fixed_window import PlaicraftMapRawDataset\n",
    "raw_dataset = PlaicraftMapRawDataset(\n",
    "    player_names=[\"Dante\"],\n",
    "    dataset_path=os.path.join(DATA_PREFIX, \"processed\"),\n",
    "    modalities=[\"video\", \"audio_in\", \"audio_out\", \"action\"],\n",
    "    window_length_frames=WINDOW_LENGTH,\n",
    "    global_database_path=Path(os.path.join(DATA_PREFIX, GLOBAL_DATABASE_SUFFIX))\n",
    ")\n",
    "\n",
    "# out_raw = raw_dataset[508]\n",
    "# key_press_raw = out_raw['action']['key_press'].transpose(-1, -2).unsqueeze(0)\n",
    "# result_raw_text = keypress_to_text(key_press_raw)[0]\n",
    "# print(result_raw_text)\n",
    "\n",
    "\n",
    "for i in range(490, 510):\n",
    "    out_raw = raw_dataset[i]\n",
    "    key_press_raw = out_raw['action']['key_press'].transpose(-1, -2).unsqueeze(0)\n",
    "    result_raw_text = keypress_to_text(key_press_raw)[0]\n",
    "    out = dataset[i]\n",
    "    key_press = out['action']['key_press'].unsqueeze(0)\n",
    "    result = decode_keypress(key_press)\n",
    "    result_text = keypress_tensor_to_text(result)\n",
    "    if [set(e) for e in result_raw_text] != [set(e) for e in result_text]:\n",
    "        print(\"=======================\")\n",
    "        print(f\"Mismatch at index {i}:\")\n",
    "        print(f\"Raw Text      : {result_raw_text}\")\n",
    "        print(f\"Processed Text: {result_text}\")\n",
    "        print(\"=======================\")\n",
    "    else:\n",
    "        print(f\"Match at index {i}: {result_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1: Key Presses: ['s', 'a'], Mouse Movements: [(0, 0)]\n",
      "Time 2: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 3: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 4: Key Presses: [], Mouse Movements: [(0, -10)]\n",
      "Time 5: Key Presses: [], Mouse Movements: [(0, -10)]\n",
      "Time 6: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 7: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 8: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 9: Key Presses: ['mouse_left'], Mouse Movements: [(0, 0)]\n",
      "Time 10: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 11: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 12: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 13: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 14: Key Presses: ['mouse_left'], Mouse Movements: [(0, 0)]\n",
      "Time 15: Key Presses: ['mouse_left'], Mouse Movements: [(0, 0)]\n",
      "Time 16: Key Presses: ['mouse_right'], Mouse Movements: [(0, 0)]\n",
      "Time 17: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 18: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 19: Key Presses: ['mouse_left'], Mouse Movements: [(0, 0)]\n",
      "Time 20: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 21: Key Presses: ['mouse_right'], Mouse Movements: [(0, 0)]\n",
      "Time 22: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 23: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 24: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 25: Key Presses: [], Mouse Movements: [(0, -20)]\n",
      "Time 26: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 27: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 28: Key Presses: [], Mouse Movements: [(0, -10)]\n",
      "Time 29: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 30: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 31: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 32: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 33: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 34: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 35: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 36: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Time 37: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 38: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 39: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 40: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 41: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 42: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 43: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 44: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 45: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 46: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 47: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 48: Key Presses: ['Shift'], Mouse Movements: [(0, 0)]\n",
      "Time 49: Key Presses: ['s'], Mouse Movements: [(0, 0)]\n",
      "Generated Key Presses: [['s', 'a'], ['s'], ['s'], [], [], ['s'], ['s'], ['Shift'], ['mouse_left'], ['s'], ['s'], ['s'], ['s'], ['mouse_left'], ['mouse_left'], ['mouse_right'], ['s'], ['s'], ['mouse_left'], ['s'], ['mouse_right'], ['s'], ['s'], ['Shift'], [], ['s'], ['s'], [], ['s'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['s'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['Shift'], ['s']]\n",
      "Generated Mouse Movements: [[(0, 0)], [(0, 0)], [(0, 0)], [(0, -10)], [(0, -10)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, -20)], [(0, 0)], [(0, 0)], [(0, -10)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "IMAGE_SIZE = (640, 360)\n",
    "\n",
    "def format_processor_image_inputs(processor_wrapper, frame) -> torch.Tensor:\n",
    "    # PLAICraft frames to JarvisVLA image inputs\n",
    "    frame = Image.fromarray(frame.permute(1, 2, 0).numpy().astype('uint8'), 'RGB')\n",
    "    return processor_wrapper.create_image_input(frame).resize(IMAGE_SIZE)\n",
    "\n",
    "\n",
    "def format_processor_keypress_inputs(converter, key_press, mouse_movement) -> str:\n",
    "    \"\"\"\n",
    "    Converts PLAICraft key presses and mouse movements to JarvisVLA text inputs.\n",
    "    Specifically, converts key_presses to a list of strings, then to JarvisVLA special tokens\n",
    "    Only processes a single 100ms key_press unit at a time (input shape: [seq_len, 79]).\n",
    "    Mouse movements within 100ms are summed (input shape: [10, 2]).\n",
    "    \"\"\"\n",
    "    key_press_text_list = keypress_to_text(key_press.unsqueeze(0))[0]\n",
    "    key_press_text = list(set(sum(key_press_text_list, [])))  # Flatten the list of lists and remove duplicates\n",
    "    mouse_movement_summed = mouse_movement.sum(dim=0, keepdim=True)  # Shape: [1, 2]\n",
    "    mouse_movement_summed[0, 0] = mouse_movement_summed[0, 0].clip(-150, 150)\n",
    "    mouse_movement_summed[0, 1] = mouse_movement_summed[0, 1].clip(-100, 100)\n",
    "    # print(f\"Key Presses (length: {len(key_press_text)}): {key_press_text}\")\n",
    "    # print(f\"Mouse Movement Summed: {mouse_movement_summed}\")\n",
    "    return converter.encode(key_press_text, mouse_movement_summed)\n",
    "\n",
    "\n",
    "def format_model_inputs(processor_wrapper, converter, frames, key_presses, mouse_movements, instruction=None):\n",
    "    instruction = instruction or \"Chop trees.\"\n",
    "    processor = processor_wrapper.processor\n",
    "    conversation = [{\"role\": \"user\",\n",
    "                     \"content\": [{\"type\": \"text\", \"text\": f\"{instruction}. \\n observation: \"},\n",
    "                                 {\"type\": \"image\", },]}]\n",
    "    # Format inputs for JarvisVLA\n",
    "    images = [format_processor_image_inputs(processor_wrapper, frames[0])]\n",
    "    if len(frames) == 1:\n",
    "        processor_prompt = processor_wrapper.create_text_input(conversation)\n",
    "        return processor(text=[processor_prompt], images=images, return_tensors=\"pt\")\n",
    "    \n",
    "    assert len(frames)-1 == len(key_presses) == len(mouse_movements), \\\n",
    "        f\"There should be 1 more frame than key and mouse data, but got {len(frames)} frames and {len(key_presses)}, {len(mouse_movements)} key and mouse data.\"\n",
    "\n",
    "    for frame, key_press, mouse_movement in zip(frames[1:], key_presses, mouse_movements):\n",
    "        images.append(format_processor_image_inputs(processor_wrapper, frame))\n",
    "        action_text = format_processor_keypress_inputs(converter, key_press, mouse_movement)\n",
    "        entry = {\"role\": \"assistant\",\n",
    "                 \"content\": [{\"type\": \"text\", \"text\": f\"{action_text} \\n observation: \"},\n",
    "                             {\"type\": \"image\", }]}\n",
    "        conversation.append(entry)\n",
    "    processor_prompt = processor_wrapper.create_text_input(conversation)\n",
    "    return processor(text=[processor_prompt], images=images, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "def generate_action(model, model_input):\n",
    "    gen_ids = model.generate(**model_input)\n",
    "    prompt_len = model_input[\"input_ids\"].shape[1]\n",
    "    return gen_ids[0, prompt_len:]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_trajectory(model, data, processor_wrapper, action_tokenizer, history_len=1, instruction=None):\n",
    "    # NOTE: Only supports batch size 1\n",
    "    all_key_presses, all_mouse_movements = [], []\n",
    "    processor = processor_wrapper.processor\n",
    "    # instruction = ' '.join([entry[0] for entry in data['transcript_in'][0]])\n",
    "\n",
    "    # Flatten batch dimension, time dimension, and 2 dimension (assumes batch size 1)\n",
    "    frames = data['video'].flatten(0, 2)\n",
    "    key_presses = data['action']['key_press'].flatten(0, 2)\n",
    "    mouse_movements = data['action']['mouse_movement'].flatten(0, 2)\n",
    "    for t in range(1, len(frames)):\n",
    "        model_input = format_model_inputs(processor_wrapper, action_tokenizer,\n",
    "                                          frames[max(t-history_len, 0):t],\n",
    "                                          key_presses[max(t-history_len, 0):t-1],\n",
    "                                          mouse_movements[max(t-history_len, 0):t-1],\n",
    "                                          instruction=instruction).to(model.device)\n",
    "        new_ids = generate_action(model, model_input)\n",
    "        new_texts = processor.decode(new_ids)\n",
    "        new_key_presses, new_mouse_movements = action_tokenizer.decode(new_texts)\n",
    "        all_key_presses.append(new_key_presses)\n",
    "        all_mouse_movements.append(new_mouse_movements)\n",
    "        print(f\"Time {t}: Key Presses: {new_key_presses}, Mouse Movements: {new_mouse_movements}\")\n",
    "    return all_key_presses, all_mouse_movements\n",
    "\n",
    "\n",
    "# Teacher forced generation\n",
    "data = raw_dataset.collate_fn([raw_dataset[0]])\n",
    "instruction = ' '.join([entry[0] for entry in data['transcript_in'][0]])\n",
    "all_key_presses, all_mouse_movements = generate_trajectory(model_causallm, data, processor_wrapper, action_tokenizer, instruction=instruction, history_len=1)\n",
    "print(\"Generated Key Presses:\", all_key_presses)\n",
    "print(\"Generated Mouse Movements:\", all_mouse_movements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 2, 3, 720, 1280]) torch.Size([1, 25, 15, 128]) torch.Size([1, 25, 15, 128]) torch.Size([1, 25, 2, 5, 79]) torch.Size([1, 25, 2, 10, 2])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Key data sequence length must be divisible by bin size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 179\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m figures\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# print(all_key_presses)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# print(data['action']['key_press'].flatten(0, 2).transpose(-1, -2).shape)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# print(keypress_tensor_to_text(decode_keypress(data['action']['key_press'].flatten(0, 2).transpose(-1, -2))))\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m figures \u001b[38;5;241m=\u001b[39m \u001b[43mplot_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkey_press\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_key_presses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmouse_movement\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_mouse_movements\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# print(figures)\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# save_figures_to_video(figures, output_path='output_video.mp4', fps=10)\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m figure \u001b[38;5;129;01min\u001b[39;00m figures:\n",
      "Cell \u001b[0;32mIn[102], line 169\u001b[0m, in \u001b[0;36mplot_all\u001b[0;34m(pred, target, max_timesteps)\u001b[0m\n\u001b[1;32m    158\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(max_timesteps, \u001b[38;5;28mlen\u001b[39m(target_video))):\n\u001b[1;32m    160\u001b[0m     decoded_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    161\u001b[0m         num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    162\u001b[0m         pred_video_frames\u001b[38;5;241m=\u001b[39mpred_video[time_index:time_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    163\u001b[0m         pred_key_press\u001b[38;5;241m=\u001b[39mpred_key_press[time_index],\n\u001b[1;32m    164\u001b[0m         pred_mouse_movement\u001b[38;5;241m=\u001b[39mpred_mouse_movement[time_index:time_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# Shape: [1, 1, 2]\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         target_video_frames\u001b[38;5;241m=\u001b[39mtarget_video[time_index:time_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    166\u001b[0m         target_transcript_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([entry[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m target_transcript_in[\u001b[38;5;241m0\u001b[39m]]),\n\u001b[1;32m    167\u001b[0m         target_transcript_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([entry[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m target_transcript_out[\u001b[38;5;241m0\u001b[39m]]),\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# target_key_press=keypress_tensor_to_text(decode_keypress(target_key_press[time_index:time_index+1].transpose(-1, -2))),\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m         target_key_press\u001b[38;5;241m=\u001b[39m\u001b[43mkeypress_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_key_press\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtime_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtime_index\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    170\u001b[0m         target_mouse_movement\u001b[38;5;241m=\u001b[39mtarget_mouse_movement[time_index:time_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),  \u001b[38;5;66;03m# Shape: [1, 1, 2]\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[1;32m    172\u001b[0m     figures\u001b[38;5;241m.\u001b[39mappend(create_validation_figure(decoded_outputs)[num_samples\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m figures\n",
      "Cell \u001b[0;32mIn[98], line 4\u001b[0m, in \u001b[0;36mkeypress_to_text\u001b[0;34m(key_data, bin_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mkeypress_to_text\u001b[39m(key_data, bin_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Input Shape: [batch_size x seq_len x 79]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Output Shape: [batch_size x seq_len x num_tokens]\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m bin_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey data sequence length must be divisible by bin size.\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m      6\u001b[0m     result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(key_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[0;31mAssertionError\u001b[0m: Key data sequence length must be divisible by bin size."
     ]
    }
   ],
   "source": [
    "# Create gridspec for better layout control\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import PIL\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def create_validation_figure(decoded_outputs):\n",
    "    \"\"\"Create matplotlib figures for multi-sample next-step prediction validation\n",
    "    \n",
    "    Creates separate visualizations for each sample showing:\n",
    "    - Top row: Predicted vs Target video frame\n",
    "    - Middle rows: Audio waveforms (predicted vs target)\n",
    "    - Bottom rows: Mouse and keyboard action comparisons\n",
    "    \n",
    "    Returns:\n",
    "        List of PIL Images, one for each sample\n",
    "    \"\"\"\n",
    "    figures = []\n",
    "    for sample_idx in range(decoded_outputs['num_samples']):\n",
    "        # Create figure with side-by-side layout for single frame comparison\n",
    "        # 2 columns (pred vs target), 6 rows (video, 2 audio, mouse, keyboard, colorbar)\n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        # Give more space to video frames (make first row taller)\n",
    "        gs = gridspec.GridSpec(6, 2, figure=fig, hspace=0.4, wspace=0.3, height_ratios=[3, 1, 1, 1.5, 2, 0.3])\n",
    "    \n",
    "        # Plot predicted video frame (left column)\n",
    "        ax_pred_video = fig.add_subplot(gs[0, 0])\n",
    "        pred_frame = decoded_outputs['pred_video_frames'][sample_idx].permute(1, 2, 0)\n",
    "        pred_frame_small = pred_frame[::2, ::2]  # Downsample by 2x (less aggressive)\n",
    "        ax_pred_video.imshow(pred_frame_small)\n",
    "        ax_pred_video.set_title('Predicted Next Frame', fontsize=14, fontweight='bold')\n",
    "        ax_pred_video.axis('off')\n",
    "        # Plot target video frame (right column)\n",
    "        ax_target_video = fig.add_subplot(gs[0, 1])\n",
    "        target_frame = decoded_outputs['target_video_frames'][sample_idx].permute(1, 2, 0)\n",
    "        target_frame_small = target_frame[::2, ::2]  # Downsample by 2x (less aggressive)\n",
    "        ax_target_video.imshow(target_frame_small)\n",
    "        ax_target_video.set_title('Target Next Frame', fontsize=14, fontweight='bold')\n",
    "        ax_target_video.axis('off')\n",
    "    \n",
    "        # Plot audio waveforms - create time axis for AUDIO_FRAMES_PER_VIDEO frames at 24kHz\n",
    "        pred_transcript_in = decoded_outputs['target_transcript_in']\n",
    "        target_transcript_in = decoded_outputs['target_transcript_in']\n",
    "        pred_transcript_out = decoded_outputs['target_transcript_out']\n",
    "        target_transcript_out = decoded_outputs['target_transcript_out']\n",
    "        # Audio IN - Predicted (left)\n",
    "        ax_pred_transcript_in = fig.add_subplot(gs[1, 0])\n",
    "        ax_pred_transcript_in.text(0.5, 0.5, pred_transcript_in, \n",
    "                                   horizontalalignment='center', verticalalignment='center', \n",
    "                                   transform=ax_pred_transcript_in.transAxes)\n",
    "        ax_pred_transcript_in.set_title('Transcript IN', fontsize=10)\n",
    "        # Audio IN - Target (right)\n",
    "        ax_target_transcript_in = fig.add_subplot(gs[1, 1])\n",
    "        ax_target_transcript_in.text(0.5, 0.5, target_transcript_in, \n",
    "                                     horizontalalignment='center', verticalalignment='center', \n",
    "                                     transform=ax_target_transcript_in.transAxes)\n",
    "        ax_target_transcript_in.set_title('Transcript IN', fontsize=10)\n",
    "        # Audio OUT - Predicted (left)\n",
    "        ax_pred_transcript_out = fig.add_subplot(gs[2, 0])\n",
    "        ax_pred_transcript_out.text(0.5, 0.5, pred_transcript_out, \n",
    "                                    horizontalalignment='center', verticalalignment='center', \n",
    "                                    transform=ax_pred_transcript_out.transAxes)\n",
    "        ax_pred_transcript_out.set_title('Predicted Transcript OUT', fontsize=10)\n",
    "        # Audio OUT - Target (right)\n",
    "        ax_target_transcript_out = fig.add_subplot(gs[2, 1])\n",
    "        ax_target_transcript_out.text(0.5, 0.5, target_transcript_out, \n",
    "                                      horizontalalignment='center', verticalalignment='center', \n",
    "                                      transform=ax_target_transcript_out.transAxes)\n",
    "        ax_target_transcript_out.set_title('Target Transcript OUT', fontsize=10)\n",
    "    \n",
    "        # Plot action data - mouse trajectory (full width)\n",
    "        ax_mouse = fig.add_subplot(gs[3, :])\n",
    "        # Get mouse data and compute cumulative trajectory\n",
    "        pred_trajectory = decoded_outputs['pred_mouse_movement'][sample_idx]\n",
    "        target_trajectory = decoded_outputs['target_mouse_movement'][sample_idx]\n",
    "        # Plot trajectories\n",
    "        # Plot predicted arrow (blue)\n",
    "        ax_mouse.annotate(\n",
    "            '', xy=pred_trajectory[0], xytext=(0, 0),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', linewidth=2),\n",
    "            label='Predicted'\n",
    "        )\n",
    "\n",
    "        # Plot target arrow (red)\n",
    "        ax_mouse.annotate(\n",
    "            '', xy=target_trajectory[0], xytext=(0, 0),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', linewidth=2),\n",
    "            label='Target'\n",
    "        )\n",
    "        ax_mouse.scatter([0], [0], color='green', s=20, marker='s', label='Start', zorder=5)\n",
    "        ax_mouse.set_title('Mouse Movement Vector (Next Timestep)', fontsize=12)\n",
    "        ax_mouse.set_xlabel('X Movement (pixels)')\n",
    "        ax_mouse.set_ylabel('Y Movement (pixels)')\n",
    "        ax_mouse.set_xlim(-150, 150)\n",
    "        ax_mouse.set_ylim(-100, 100)\n",
    "        legend_handles = [\n",
    "            Line2D([0], [0], color='blue', lw=2, label='Predicted'),\n",
    "            Line2D([0], [0], color='red', lw=2, label='Target'),\n",
    "            Line2D([0], [0], marker='s', color='green', linestyle='None', markersize=8, label='Start')\n",
    "        ]\n",
    "        ax_mouse.legend(handles=legend_handles)\n",
    "        ax_mouse.grid(True, alpha=0.3)\n",
    "\n",
    "        # Keyboard state heatmaps - predicted (left) vs target (right)\n",
    "        pred_keyboard_first = decoded_outputs['pred_key_press']\n",
    "        target_keyboard_first = decoded_outputs['target_key_press'][sample_idx]\n",
    "        # Predicted keyboard states (left)\n",
    "        ax_keys_pred = fig.add_subplot(gs[4, 0])\n",
    "        ax_keys_pred.text(0.5, 0.5, pred_keyboard_first, fontsize=16,\n",
    "                          horizontalalignment='center', verticalalignment='center', \n",
    "                          transform=ax_keys_pred.transAxes)\n",
    "        ax_keys_pred.set_title('Pressed Keys', fontsize=10)\n",
    "        ax_keys_pred.set_xticks([])\n",
    "        ax_keys_pred.set_yticks([])\n",
    "        # Target keyboard states (right)\n",
    "        ax_keys_target = fig.add_subplot(gs[4, 1])\n",
    "        # print(decoded_outputs['target_key_press'][sample_idx].shape)\n",
    "        # im_target = ax_keys_target.imshow(target_keyboard_first.T, aspect='auto', cmap='YlOrRd', vmin=0, vmax=1)\n",
    "        ax_keys_target.text(0.5, 0.5, target_keyboard_first, fontsize=16,\n",
    "                            horizontalalignment='center', verticalalignment='center', \n",
    "                            transform=ax_keys_target.transAxes)\n",
    "        ax_keys_target.set_title('Pressed Keys', fontsize=10)\n",
    "        ax_keys_target.set_xticks([])\n",
    "        ax_keys_target.set_yticks([])\n",
    "        \n",
    "        # Add shared colorbar (full width at bottom)\n",
    "        cbar_ax = fig.add_subplot(gs[5, :])\n",
    "        cbar_ax.axis('off')        \n",
    "        plt.suptitle(f'Validation Sample {sample_idx + 1}: Next-Step Prediction', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        # Convert to image for WandB\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "        img = PIL.Image.open(buf)\n",
    "        plt.close(fig)\n",
    "        figures.append(img)\n",
    "    return figures\n",
    "\n",
    "\n",
    "print(data['video'].shape, data['audio_in'].shape, data['audio_out'].shape, data['action']['key_press'].shape, data['action']['mouse_movement'].shape)\n",
    "\n",
    "\n",
    "def plot_all(pred, target, max_timesteps=10):\n",
    "    # NOTE: Only works with batch size = 1\n",
    "    target_video = target['video'].flatten(0, 2)\n",
    "    target_transcript_in = target['transcript_in']\n",
    "    target_transcript_out = target['transcript_out']\n",
    "    # target_key_press = target['action']['key_press'].flatten(0, 2)\n",
    "    target_key_press = target['action']['key_press'] # .flatten(1, 3)\n",
    "    target_mouse_movement = target['action']['mouse_movement'].flatten(0, 2)\n",
    "\n",
    "    pred_video = torch.zeros_like(target_video)\n",
    "    pred_key_press = pred['key_press']\n",
    "    pred_mouse_movement = torch.tensor(pred['mouse_movement'])\n",
    "\n",
    "    figures = []\n",
    "    num_samples = 1\n",
    "    for time_index in range(min(max_timesteps, len(target_video))):\n",
    "        decoded_outputs = dict(\n",
    "            num_samples=1,\n",
    "            pred_video_frames=pred_video[time_index:time_index+1],\n",
    "            pred_key_press=pred_key_press[time_index],\n",
    "            pred_mouse_movement=pred_mouse_movement[time_index:time_index+1].transpose(0, 1),  # Shape: [1, 1, 2]\n",
    "            target_video_frames=target_video[time_index:time_index+1],\n",
    "            target_transcript_in=' '.join([entry[0] for entry in target_transcript_in[0]]),\n",
    "            target_transcript_out=' '.join([entry[0] for entry in target_transcript_out[0]]),\n",
    "            # target_key_press=keypress_tensor_to_text(decode_keypress(target_key_press[time_index:time_index+1].transpose(-1, -2))),\n",
    "            target_key_press=keypress_to_text(target_key_press[:, time_index:time_index+1].flatten(1, 3))[0],\n",
    "            target_mouse_movement=target_mouse_movement[time_index:time_index+1].sum(dim=-2, keepdim=True),  # Shape: [1, 1, 2]\n",
    "        )\n",
    "        figures.append(create_validation_figure(decoded_outputs)[num_samples-1])\n",
    "    return figures\n",
    "\n",
    "\n",
    "# print(all_key_presses)\n",
    "# print(data['action']['key_press'].flatten(0, 2).transpose(-1, -2).shape)\n",
    "# print(keypress_tensor_to_text(decode_keypress(data['action']['key_press'].flatten(0, 2).transpose(-1, -2))))\n",
    "figures = plot_all(pred={'key_press': all_key_presses, 'mouse_movement': all_mouse_movements}, target=data)\n",
    "# print(figures)\n",
    "# save_figures_to_video(figures, output_path='output_video.mp4', fps=10)\n",
    "\n",
    "for figure in figures:\n",
    "    display(figure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
